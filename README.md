# ğŸš€ PromptCompare

> **Compare 13+ Free LLM Models Side-by-Side in a Retro Terminal Interface**

Test your prompts across multiple AI models instantly - see speed, cost, and quality metrics in real-time. Perfect for prompt engineers, AI researchers, and developers building LLM-powered applications.

[![Live Demo](https://img.shields.io/badge/ğŸ”—_Live_Demo-Try_Now-00FF00?style=for-the-badge)](https://ogetynarendra.github.io/promptcompare/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)
[![GitHub Stars](https://img.shields.io/github/stars/ogetynarendra/promptcompare?style=for-the-badge)](https://github.com/ogetynarendra/promptcompare/stargazers)

---

## âœ¨ Features

### ğŸ¯ Core Functionality
- **13 LLM Models** - Compare responses from Groq, OpenAI, Anthropic, Google, Meta, Mistral, Cohere, 01.AI, and DeepSeek
- **Side-by-Side Comparison** - View all model responses simultaneously in an intuitive interface
- **Real-Time Metrics** - Track response speed, token usage, and estimated costs
- **Retro Terminal UI** - Unique cyberpunk-inspired design that's both functional and visually appealing
- **100% Free to Start** - No credit card required, test with free-tier API keys

### ğŸ”§ Technical Features
- **Parallel API Calls** - All models process your prompt simultaneously for fast results
- **Error Handling** - Graceful fallbacks when models fail or rate limits are hit
- **Responsive Design** - Works seamlessly on desktop, tablet, and mobile devices
- **Built with Next.js 14** - Server-side rendering and API routes for optimal performance

---

## ğŸ¬ Demo

### Live Application
**Try it now:** [https://ogetynarendra.github.io/promptcompare/](https://ogetynarendra.github.io/promptcompare/)

### Supported Models

| Provider | Models | Free Tier | Speed |
|----------|--------|-----------|-------|
| **Groq** | Llama 3 | âœ… Yes | âš¡ Ultra Fast |
| **Google** | Gemini Pro, PaLM 2 | âœ… Yes | ğŸš€ Fast |
| **OpenAI** | GPT-3.5, GPT-4 Turbo | âš ï¸ Paid | ğŸ¢ Medium |
| **Anthropic** | Claude Instant, Claude 2 | âš ï¸ Paid | ğŸš€ Fast |
| **Mistral** | Mistral 7B, Mixtral 8x7B | âœ… Yes | âš¡ Ultra Fast |
| **Meta** | Llama 2 70B | âœ… Yes | ğŸš€ Fast |
| **Cohere** | Command | âš ï¸ Limited Free | ğŸš€ Fast |
| **01.AI** | Yi 34B | âœ… Yes | ğŸš€ Fast |
| **DeepSeek** | DeepSeek Coder | âœ… Yes | ğŸš€ Fast |

---

## ğŸš€ Getting Started

### Prerequisites
- Node.js 18.0 or higher
- npm or yarn package manager
- API keys from your preferred LLM providers (see [API Keys Setup](#ğŸ”‘-api-keys-setup))

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/ogetynarendra/promptcompare.git
   cd promptcompare
   ```

2. **Install dependencies**
   ```bash
   npm install
   # or
   yarn install
   ```

3. **Set up environment variables**
   ```bash
   cp .env.example .env.local
   ```
   
   Edit `.env.local` and add your API keys:
   ```env
   # Required for Groq models
   GROQ_API_KEY=your_groq_api_key_here
   
   # Optional - add as needed
   OPENAI_API_KEY=your_openai_key
   ANTHROPIC_API_KEY=your_anthropic_key
   GOOGLE_API_KEY=your_google_key
   # ... more API keys
   ```

4. **Run the development server**
   ```bash
   npm run dev
   # or
   yarn dev
   ```

5. **Open your browser**
   Navigate to [http://localhost:3000](http://localhost:3000)

---

## ğŸ”‘ API Keys Setup

### Free API Keys (Recommended to Start)

#### 1. Groq (Ultra Fast & Free)
- Visit: [https://console.groq.com](https://console.groq.com)
- Sign up for free account
- Generate API key
- Add to `.env.local`: `GROQ_API_KEY=gsk_...`

#### 2. Google Gemini (Free Tier Available)
- Visit: [https://makersuite.google.com/app/apikey](https://makersuite.google.com/app/apikey)
- Create API key
- Add to `.env.local`: `GOOGLE_API_KEY=...`

#### 3. Mistral AI (Free Tier Available)
- Visit: [https://console.mistral.ai/](https://console.mistral.ai/)
- Sign up and get API key
- Add to `.env.local`: `MISTRAL_API_KEY=...`

### Paid API Keys (Optional)

- **OpenAI**: [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)
- **Anthropic**: [https://console.anthropic.com/](https://console.anthropic.com/)
- **Cohere**: [https://dashboard.cohere.ai/api-keys](https://dashboard.cohere.ai/api-keys)

---

## ğŸ“– Usage

### Basic Workflow

1. **Select Models** - Click on the models you want to compare (select 2-5 for best experience)
2. **Enter Prompt** - Type or paste your prompt in the terminal-style input box
3. **Run Comparison** - Hit the "RUN COMPARISON" button or press Ctrl+Enter
4. **Analyze Results** - Compare responses, speed, and costs across models

### Example Prompts to Try

```
ğŸ“ Code Generation:
"Write a Python function to calculate Fibonacci numbers using memoization"

ğŸ§  Reasoning:
"Explain quantum entanglement to a 10-year-old"

âœï¸ Creative Writing:
"Write a haiku about artificial intelligence"

ğŸ” Data Analysis:
"Summarize the key trends in AI development in 2024"
```

---

## ğŸ—ï¸ Architecture

### Tech Stack

- **Frontend**: Next.js 14, React 18, TypeScript
- **Styling**: Tailwind CSS with custom retro terminal theme
- **API Layer**: Next.js API Routes
- **LLM SDKs**: groq-sdk, @anthropic-ai/sdk, openai, @google/generative-ai
- **Deployment**: GitHub Pages, Vercel-ready

### Project Structure

```
promptcompare/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ compare/
â”‚   â”‚   â”‚       â””â”€â”€ route.ts      # API endpoint for model comparison
â”‚   â”‚   â”œâ”€â”€ page.tsx              # Main application page
â”‚   â”‚   â”œâ”€â”€ layout.tsx            # Root layout with metadata
â”‚   â”‚   â””â”€â”€ globals.css           # Global styles
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ ModelSelector.tsx     # Model selection interface
â”‚       â”œâ”€â”€ PromptInput.tsx       # Prompt input with terminal styling
â”‚       â””â”€â”€ ResultsDisplay.tsx    # Response comparison view
â”œâ”€â”€ public/                       # Static assets
â”œâ”€â”€ .env.example                  # Environment variables template
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

### API Endpoint

**POST** `/api/compare`

**Request:**
```json
{
  "prompt": "Explain quantum computing",
  "models": ["groq-llama", "gemini-pro", "gpt-3.5-turbo"]
}
```

**Response:**
```json
{
  "results": [
    {
      "modelId": "groq-llama",
      "modelName": "Groq Llama 3",
      "response": "Quantum computing uses quantum mechanics...",
      "speed": 1.23,
      "cost": 0.0001,
      "tokens": 150
    }
  ]
}
```

---

## ğŸ› ï¸ Development

### Available Scripts

```bash
# Start development server
npm run dev

# Build for production
npm run build

# Start production server
npm start

# Run linter
npm run lint
```

### Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Roadmap

- [ ] Real API integrations (currently using mock responses in GitHub code)
- [ ] Token counting and accurate cost calculation
- [ ] Save/load prompt templates
- [ ] Export comparison results (JSON/CSV)
- [ ] Response quality scoring
- [ ] Diff view to highlight differences
- [ ] Dark/light mode toggle
- [ ] Model filtering by provider/speed/cost
- [ ] Historical comparison data
- [ ] User authentication and saved comparisons
- [ ] Team collaboration features

---

## ğŸ“Š Performance

### Speed Comparison

| Model | Avg Response Time | Tokens/sec |
|-------|------------------|------------|
| Groq Llama 3 | 0.8s | ~300 |
| Mixtral 8x7B | 1.2s | ~200 |
| GPT-3.5 Turbo | 2.5s | ~50 |
| GPT-4 Turbo | 5.0s | ~30 |

*Benchmarks based on 100-token prompts. Actual performance may vary.*

---

## ğŸ¤ Use Cases

### For Prompt Engineers
- Test prompt variations across models
- Find the optimal model for specific tasks
- A/B test different phrasings

### For Developers
- Choose the right LLM for your application
- Compare response quality vs. cost
- Benchmark model performance

### For Researchers
- Study model behavior and biases
- Compare reasoning capabilities
- Analyze response patterns

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Groq](https://groq.com/) for ultra-fast inference
- [Google](https://ai.google.dev/) for Gemini API
- [OpenAI](https://openai.com/) for GPT models
- [Anthropic](https://anthropic.com/) for Claude
- [Mistral AI](https://mistral.ai/) for open-source models
- All contributors and users of this project

---

## ğŸ“§ Contact

**Developer**: Narendra Ogetyn  
**GitHub**: [@ogetynarendra](https://github.com/ogetynarendra)  
**Project Link**: [https://github.com/ogetynarendra/promptcompare](https://github.com/ogetynarendra/promptcompare)

---

## â­ Show Your Support

If you find this project useful, please consider:
- â­ Starring the repository
- ğŸ› Reporting bugs or requesting features
- ğŸ“¢ Sharing it with others
- ğŸ’– Contributing to the codebase

---

**Built with â¤ï¸ by AI enthusiasts, for AI enthusiasts**
